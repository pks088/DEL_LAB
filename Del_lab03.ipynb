{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e625816-7696-43fc-8210-df723f507296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10000, Loss: 1.182035254895409\n",
      "Epoch 1000/10000, Loss: 0.0696812182487262\n",
      "Epoch 2000/10000, Loss: 0.05597464075240986\n",
      "Epoch 3000/10000, Loss: 0.05206132920787736\n",
      "Epoch 4000/10000, Loss: 0.05044812012567026\n",
      "Epoch 5000/10000, Loss: 0.049474717553755106\n",
      "Epoch 6000/10000, Loss: 0.04874105959947561\n",
      "Epoch 7000/10000, Loss: 0.04810424444815263\n",
      "Epoch 8000/10000, Loss: 0.047491408593254396\n",
      "Epoch 9000/10000, Loss: 0.04685860963515138\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.001, epochs=10000):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # Initialize weights and biases with He initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.hidden_input = np.dot(X, self.W1) + self.b1\n",
    "        self.hidden_output = relu(self.hidden_input)\n",
    "        self.output_input = np.dot(self.hidden_output, self.W2) + self.b2\n",
    "        self.output = softmax(self.output_input)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        # Compute errors and deltas\n",
    "        output_error = self.output - y\n",
    "        output_delta = output_error\n",
    "        hidden_error = output_delta.dot(self.W2.T)\n",
    "        hidden_delta = hidden_error * relu_derivative(self.hidden_output)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.W2 -= self.learning_rate * self.hidden_output.T.dot(output_delta)\n",
    "        self.b2 -= self.learning_rate * np.sum(output_delta, axis=0, keepdims=True)\n",
    "        self.W1 -= self.learning_rate * X.T.dot(hidden_delta)\n",
    "        self.b1 -= self.learning_rate * np.sum(hidden_delta, axis=0, keepdims=True)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.forward(X)\n",
    "            self.backward(X, y)\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                # Compute and print loss every 1000 epochs\n",
    "                loss = -np.mean(np.sum(y * np.log(self.output + 1e-8), axis=1))\n",
    "                print(f'Epoch {epoch}/{self.epochs}, Loss: {loss}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the MLP model\n",
    "mlp = MLP(input_size=4, hidden_size=5, output_size=3, learning_rate=0.001, epochs=10000)\n",
    "mlp.train(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = mlp.predict(X_test)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8097dbf-f502-4d70-88be-8df74e56132a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
